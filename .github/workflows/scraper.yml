name: "Adobe Security Bulletins Scraper"

on:
  schedule:
    # Run every 6 hours (at 00:00, 06:00, 12:00, and 18:00 UTC)
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      scraper_method:
        description: 'Scraping method to use'
        required: true
        default: 'auto'
        type: choice
        options:
          - 'auto'
          - 'manual'
      bulletin_data:
        description: 'Manual bulletin data (when method=manual, paste table format data)'
        required: false
        default: ''
        type: string
      force_update:
        description: 'Force update all bulletins (ignore cache)'
        required: false
        default: false
        type: boolean
  push:
    branches: ["main"]
    paths: 
      - "cmd/adobe-scraper/**"
      - "cmd/content-generator/**" 
      - "internal/**"
      - ".github/workflows/scraper.yml"
  
permissions:
  contents: write  # Allow commits back to main

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'
          
      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-
            
      - name: Install dependencies
        run: go mod download
        
      - name: Create required directories
        run: |
          mkdir -p content/bulletins
          mkdir -p static/feeds
          
      - name: Run automated scraper
        if: ${{ github.event.inputs.scraper_method != 'manual' }}
        run: |
          echo "ü§ñ Starting Adobe Security Bulletins automated scraper..."
          go run cmd/adobe-scraper/main.go auto
        env:
          TZ: UTC
          
      - name: Run manual parser
        if: ${{ github.event.inputs.scraper_method == 'manual' && github.event.inputs.bulletin_data != '' }}
        run: |
          echo "üìù Processing manual bulletin data..."
          echo "${{ github.event.inputs.bulletin_data }}" | go run cmd/adobe-scraper/main.go manual
          
      - name: Generate Hugo content
        run: |
          echo "üèóÔ∏è  Generating Hugo content from bulletins database..."
          go run cmd/content-generator/main.go generate
          
      - name: Check for changes
        id: changes
        run: |
          git add .
          if git diff --staged --quiet; then
            echo "changes=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No changes detected - no new bulletins found"
          else
            echo "changes=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Changes detected:"
            git diff --staged --name-only
            echo ""
            echo "üìä Summary:"
            echo "- Bulletin database: $(git diff --staged --name-only | grep -c "security-bulletins.json" || echo "0") files"
            echo "- Content files: $(git diff --staged --name-only | grep -c "\.md$" || echo "0") files"  
            echo "- Static files: $(git diff --staged --name-only | grep -E "(\.xml|\.json)$" | grep -v security-bulletins.json | wc -l || echo "0") files"
          fi
          
      - name: Commit and push changes
        if: steps.changes.outputs.changes == 'true'
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Count new/modified files
          NEW_FILES=$(git diff --staged --name-only | grep "\.md$" | wc -l)
          RSS_FILES=$(git diff --staged --name-only | grep "\.xml$" | wc -l)
          
          # Create commit message based on trigger type
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            if [ "${{ github.event.inputs.scraper_method }}" = "manual" ]; then
              COMMIT_MSG="Manual bulletin data import - $(date -u +"%Y-%m-%d %H:%M UTC")"
            else
              COMMIT_MSG="Manual scraper trigger - $(date -u +"%Y-%m-%d %H:%M UTC")"
            fi
          else
            COMMIT_MSG="Automated bulletin update - $(date -u +"%Y-%m-%d %H:%M UTC")"
          fi
          
          if [ "$NEW_FILES" -gt 0 ]; then
            COMMIT_MSG="${COMMIT_MSG}

          üìÑ Content files: ${NEW_FILES}"
          fi
          if [ "$RSS_FILES" -gt 0 ]; then
            COMMIT_MSG="${COMMIT_MSG}
          üóûÔ∏è  RSS feeds: ${RSS_FILES}"
          fi
          
          # Add trigger-specific info
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            COMMIT_MSG="${COMMIT_MSG}
          ü§ñ Method: ${{ github.event.inputs.scraper_method }}"
            if [ "${{ github.event.inputs.force_update }}" = "true" ]; then
              COMMIT_MSG="${COMMIT_MSG}
          üîÑ Force update enabled"
            fi
          fi
          
          git commit -m "$COMMIT_MSG"
          git push
          
      - name: Report results
        if: always()
        run: |
          if [ "${{ steps.changes.outputs.changes }}" = "true" ]; then
            echo "‚úÖ Scraper completed successfully with updates"
            echo "Changes have been committed and pushed to main branch"
            echo "The Hugo deployment workflow will be triggered automatically"
          else
            echo "‚ÑπÔ∏è  Scraper completed successfully with no updates"
            echo "No new or modified security bulletins found"
          fi
          
      - name: Upload cache as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-cache
          path: .scraper-cache.json
          retention-days: 30
          
      - name: Upload logs as artifact on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: |
            *.log
            .scraper-cache.json
          retention-days: 7